{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Io_PW41ZW4XU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import imdb\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras import losses\n",
        "from keras import metrics\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data, keeping only 10,000 of the most frequently occuring words\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = 10000)"
      ],
      "metadata": {
        "id": "xuEth1hWW6Ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the first label\n",
        "train_labels[0]"
      ],
      "metadata": {
        "id": "GSPL0XgFW9dQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Since we restricted ourselves to the top 10000 frequent words, no word index should exceed 10000\n",
        "# we'll verify this below\n",
        "\n",
        "# Here is a list of maximum indexes in every review --- we search the maximum index in this list of max indexes\n",
        "print(type([max(sequence) for sequence in train_data]))\n",
        "\n",
        "# Find the maximum of all max indexes\n",
        "max([max(sequence) for sequence in train_data])"
      ],
      "metadata": {
        "id": "UWbW7htAW_Fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's quickly decode a review\n",
        "\n",
        "# step 1: load the dictionary mappings from word to integer index\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# step 2: reverse word index to map integer indexes to their respective words\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "# Step 3: decode the review, mapping integer indices to words\n",
        "#\n",
        "# indices are off by 3 because 0, 1, and 2 are reserverd indices for \"padding\", \"Start of sequence\" and \"unknown\"\n",
        "decoded_review = ' '.join([reverse_word_index.get(i-3, '?') for i in train_data[0]])\n",
        "\n",
        "decoded_review"
      ],
      "metadata": {
        "id": "E9RWC5SWW_jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(reverse_word_index)"
      ],
      "metadata": {
        "id": "ZYkyg-TAXB0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))    # Creates an all zero matrix of shape (len(sequences),10K)\n",
        "    for i,sequence in enumerate(sequences):\n",
        "        results[i,sequence] = 1                        # Sets specific indices of results[i] to 1s\n",
        "    return results\n",
        "\n",
        "# Vectorize training Data\n",
        "X_train = vectorize_sequences(train_data)\n",
        "\n",
        "# Vectorize testing Data\n",
        "X_test = vectorize_sequences(test_data)"
      ],
      "metadata": {
        "id": "gZt1KHs2XD3S"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0]"
      ],
      "metadata": {
        "id": "dTLgjoQDXGN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "HKbIQUsVXJFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test  = np.asarray(test_labels).astype('float32')"
      ],
      "metadata": {
        "id": "kgFT1-C0XK00"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Defination\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "lMoAVahKXMxE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile Model\n",
        "model.compile(\n",
        "    optimizer=optimizers.RMSprop(learning_rate=0.001),\n",
        "    loss = losses.binary_crossentropy,\n",
        "    metrics = [metrics.binary_accuracy]\n",
        ")"
      ],
      "metadata": {
        "id": "ujNsr3NMXRBT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input for Validation\n",
        "X_val = X_train[:10000]\n",
        "partial_X_train = X_train[10000:]\n",
        "\n",
        "# Labels for validation\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]"
      ],
      "metadata": {
        "id": "q4iY783iXVMF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "history = model.fit(\n",
        "    partial_X_train,\n",
        "    partial_y_train,\n",
        "    epochs=20,\n",
        "    batch_size=512,\n",
        "    validation_data=(X_val, y_val)\n",
        ")"
      ],
      "metadata": {
        "id": "pwx06ui3XXbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "metadata": {
        "id": "UXk-hJd8XbPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting losses\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "\n",
        "plt.plot(epochs, loss_values, 'g', label=\"Training Loss\")\n",
        "plt.plot(epochs, val_loss_values, 'b', label=\"Validation Loss\")\n",
        "\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss Value')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6hJl-izzXe61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and Validation Accuracy\n",
        "\n",
        "acc_values = history_dict['binary_accuracy']\n",
        "val_acc_values = history_dict['val_binary_accuracy']\n",
        "\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "\n",
        "plt.plot(epochs, acc_values, 'g', label=\"Training Accuracy\")\n",
        "plt.plot(epochs, val_acc_values, 'b', label=\"Validation Accuracy\")\n",
        "\n",
        "plt.title('Training and Validation Accuraccy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "40nNb6r-XhQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retraining the model\n",
        "model.fit(\n",
        "    partial_X_train,\n",
        "    partial_y_train,\n",
        "    epochs=3,\n",
        "    batch_size=512,\n",
        "    validation_data=(X_val, y_val)\n",
        ")"
      ],
      "metadata": {
        "id": "5kP6VV_vXjRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model evaluation\n",
        "# Making Predictions for testing data\n",
        "np.set_printoptions(suppress=True)\n",
        "result = model.predict(X_test)"
      ],
      "metadata": {
        "id": "ULqm_LnCXn_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "4DVCplMhXr8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.zeros(len(result))\n",
        "for i, score in enumerate(result):\n",
        "    y_pred[i] = np.round(score)"
      ],
      "metadata": {
        "id": "wr8_Qn8NXuCl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mae = metrics.mean_absolute_error(y_pred, y_test)\n",
        "mae"
      ],
      "metadata": {
        "id": "fqSI2W-OXvv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZAJLK8_oXxW3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}